{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3ddef90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca5aee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "from seleniumwire import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "from selenium.common.exceptions import (\n",
    "    MoveTargetOutOfBoundsException,\n",
    "    TimeoutException,\n",
    "    WebDriverException,\n",
    ")\n",
    "\n",
    "import chromedriver_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32bf04b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('kmdb/', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea40c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_browser():\n",
    "    \"\"\"\n",
    "    Opens a new automated browser window with all tell-tales of automated browser disabled\n",
    "    \"\"\"\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"start-maximized\")\n",
    "    \n",
    "    # remove all signs of this being an automated browser\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option('useAutomationExtension', False)\n",
    "\n",
    "    # open the browser with the new options\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e2ce96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f12887bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the url as one allowing to enter straight onto the politcs forum of the website\n",
    "url = 'https://www.kmdb.or.kr/main'\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa0e7e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnifier = driver.find_element(By.XPATH,\n",
    "                               './/button[@id=\"search-btn-m\"]')\n",
    "magnifier.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3959a884",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_box = driver.find_element(\n",
    "    By.XPATH, \n",
    "    './/input[@id=\"headerDesktopInput\"]'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5db4d41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = '조선족'\n",
    "search_box.send_keys(search_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95ac3b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def press_enter(driver):\n",
    "    \"\"\"\n",
    "    Sends the ENTER to a webdriver instance.\n",
    "    \"\"\"\n",
    "    actions = ActionChains(driver)\n",
    "    actions.send_keys(Keys.ENTER)\n",
    "    actions.perform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfd6af17",
   "metadata": {},
   "outputs": [],
   "source": [
    "press_enter(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "678359fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_more = driver.find_element(By.XPATH, \n",
    "                               './/a[@class=\"iMore1\"]')\n",
    "show_more.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a33aee66",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_source = driver.page_source\n",
    "\n",
    "# Save the page source to a file\n",
    "with open('kmdb/page_source.html', 'w', encoding='utf-8') as file:\n",
    "    file.write(page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15a3c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results_on_page(driver, fn_out):\n",
    "    \"\"\"\n",
    "    Scrolls to load all listings and then saves them to `fn_out`.\n",
    "    If you found a better approach, replace this function\n",
    "    \"\"\"\n",
    "    N = 0\n",
    "    while True:\n",
    "        # get all the listings, and scroll to the last one, then wait two seconds.\n",
    "        posts = driver.find_elements(By.XPATH, './/li[@class=\"detail-box\"]')\n",
    "        last_post = posts[-1]\n",
    "\n",
    "        # you can use selenium to issue JavaScript commands:\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView();\", last_post)\n",
    "        N_posts = len(posts)\n",
    "        if N_posts == N:\n",
    "            break\n",
    "        N = N_posts\n",
    "        time.sleep(2)\n",
    "        \n",
    "    # how to save what the emulator sees\n",
    "    with open(fn_out, 'w') as f:\n",
    "        f.write(driver.page_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2540a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "    fn_out = 'kmdb/kmbd_choshun_page_1.html'\n",
    "    get_results_on_page(driver, fn_out)\n",
    "    next_page = driver.find_element(By.XPATH, \".//a[@href = \\\"javascript:goPage('10');\\\"]\")\n",
    "    next_page.click()\n",
    "    fn_out = 'kmdb/kmbd_choshun_page_2.html'\n",
    "    get_results_on_page(driver, fn_out)\n",
    "    next_page = driver.find_element(By.XPATH, \".//a[@href = \\\"javascript:goPage('20');\\\"]\")\n",
    "    next_page.click()\n",
    "    fn_out = 'kmdb/kmbd_choshun_page_3.html'\n",
    "    get_results_on_page(driver, fn_out)\n",
    "    next_page = driver.find_element(By.XPATH, \".//a[@href = \\\"javascript:goPage('30');\\\"]\")\n",
    "    next_page.click()\n",
    "    fn_out = 'kmdb/kmbd_choshun_page_4.html'\n",
    "    get_results_on_page(driver, fn_out)\n",
    "    next_page = driver.find_element(By.XPATH, \".//a[@href = \\\"javascript:goPage('40');\\\"]\")\n",
    "    next_page.click()\n",
    "    fn_out = 'kmdb/kmbd_choshun_page_5.html'\n",
    "    get_results_on_page(driver, fn_out)\n",
    "    next_page_button = driver.find_element(By.XPATH,\n",
    "                                              './/a[@class=\"btn next\"]')\n",
    "    next_page_button.click()\n",
    "    fn_out = 'kmdb/kmbd_choshun_page_6.html'\n",
    "    get_results_on_page(driver, fn_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b54e2312",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad229d0",
   "metadata": {},
   "source": [
    "### To parse and extract data from multiple HTML files using BeautifulSoup, you can follow these steps:\n",
    "\n",
    "1. Read the HTML files: Read the content of each HTML file and store it as a string.\n",
    "\n",
    "2. Create BeautifulSoup objects: Convert each HTML string into a BeautifulSoup object.\n",
    "\n",
    "3. Extract data: Use BeautifulSoup methods to navigate and extract data from each BeautifulSoup object.\n",
    "\n",
    "Here's a Python code example to demonstrate this process:\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Assuming you have a list of file paths for the HTML files\n",
    "html_files = [\n",
    "    'file1.html',\n",
    "    'file2.html',\n",
    "    'file3.html',\n",
    "    # Add more file paths here as needed\n",
    "]\n",
    "\n",
    "def extract_data_from_html(html_content):\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Extract and process data from the BeautifulSoup object\n",
    "    # Modify this section based on the structure of your HTML files\n",
    "    # For example:\n",
    "    # data = soup.find('div', class_='content').text\n",
    "    # return data\n",
    "\n",
    "# Loop through each HTML file and process its content\n",
    "for file_path in html_files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "        data = extract_data_from_html(html_content)\n",
    "        # Do something with the extracted data, e.g., store it in a list or database\n",
    "```\n",
    "\n",
    "In the above code, we first define a function `extract_data_from_html` to extract the required data from each BeautifulSoup object. You will need to modify this function based on the specific structure of your HTML files.\n",
    "\n",
    "Then, we loop through each HTML file, read its content, create a BeautifulSoup object for each file, and call the `extract_data_from_html` function to extract the data.\n",
    "\n",
    "Please note that the exact parsing and data extraction will depend on the structure of your HTML files and the specific data you want to extract. Modify the `extract_data_from_html` function according to the HTML structure and data you are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "57c709d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'kmbd_choshun_page_4.html'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Loop through each HTML file and process its content\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m html_files:\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     31\u001b[0m         html_content \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     32\u001b[0m         data \u001b[38;5;241m=\u001b[39m extract_data_from_html(html_content)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/lib/python3.10/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'kmbd_choshun_page_4.html'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Folder path containing the HTML files\n",
    "folder_path = 'kmdb'\n",
    "\n",
    "# Assuming you have a list of file paths for the HTML files\n",
    "html_files = []\n",
    "for p in range(1,7):\n",
    "    html = f'kmbd_choshun_page_{p}.html'\n",
    "    html_files.append(html)\n",
    "\n",
    "def extract_data_from_html(html_content):\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    big_box = soup.select(\"ul\")\n",
    "    detail_boxes = big_box.select(\"li\")\n",
    "    box1 = detail_boxes.select(\".details\")\n",
    "    for b in box1:\n",
    "        return\n",
    "        eng_title = b.select(\"a div.movie-tt-eng searchFont\").text\n",
    "        genre = b.select(\"div a span\").text\n",
    "        etc = b.select(\"div span\").text\n",
    "        \n",
    "# Get a list of file names in the 'kmdb' folder\n",
    "html_files = [f for f in os.listdir(folder_path) if f.endswith('.html')]\n",
    "\n",
    "# Loop through each HTML file and process its content\n",
    "for file_path in html_files:\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "        data = extract_data_from_html(html_content)\n",
    "        # Do something with the extracted data, e.g., store it in a list or database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d338e89",
   "metadata": {},
   "source": [
    "### Yes, there is an issue with your `extract_data_from_html` function. The problem is that you are returning immediately after entering the `for` loop, so the function will terminate without executing any code after the `return` statement. As a result, nothing will be returned from the function.\n",
    "\n",
    "To fix this, you need to remove the `return` statement that appears at the beginning of the `for` loop. Instead, you should create a data structure (e.g., a list or dictionary) to store the extracted data from each iteration of the loop and return that data structure after the loop completes.\n",
    "\n",
    "Here's the updated `extract_data_from_html` function:\n",
    "\n",
    "```python\n",
    "def extract_data_from_html(html_content):\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    big_box = soup.select(\"ul\")\n",
    "    detail_boxes = big_box.select(\"li\")\n",
    "    \n",
    "    # Create a list to store the extracted data from each detail box\n",
    "    extracted_data = []\n",
    "    \n",
    "    for b in detail_boxes:\n",
    "        # Extract data from each detail box\n",
    "        eng_title = b.select(\"a div.movie-tt-eng.searchFont\").text.strip()\n",
    "        genre = b.select(\"div a span\").text.strip()\n",
    "        etc = b.select(\"div span\").text.strip()\n",
    "        \n",
    "        # Create a dictionary to store the extracted data from this detail box\n",
    "        data_dict = {\n",
    "            'eng_title': eng_title,\n",
    "            'genre': genre,\n",
    "            'etc': etc\n",
    "        }\n",
    "        \n",
    "        # Append the data_dict to the extracted_data list\n",
    "        extracted_data.append(data_dict)\n",
    "    \n",
    "    # Return the list containing all the extracted data from the HTML\n",
    "    return extracted_data\n",
    "```\n",
    "\n",
    "With this updated function, it will now properly extract and store the data from each detail box in the `extracted_data` list. The `extracted_data` list will be returned as the result of the function, and you can use it to store or process the data as needed in the loop that calls this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d32e14c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_data_from_html(html_content):\n",
    "    # Create a BeautifulSoup object\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Find all the li elements with class \"detail-box\"\n",
    "    detail_boxes = soup.select(\"li.detail-box\")\n",
    "    \n",
    "    # Create a list to store the extracted data from each detail box\n",
    "    extracted_data = []\n",
    "    \n",
    "    # Loop through each li element (detail box)\n",
    "    for box in detail_boxes:\n",
    "        eng_title = box.select_one(\"a div.movie-tt-eng\").text.strip()\n",
    "        genre_elements = box.select(\"div.national span\")\n",
    "        genre = genre_elements[0].text.strip() if genre_elements else \"\"\n",
    "        year_elements = box.select(\"div.year span\")\n",
    "        year = year_elements[1].text.strip() if year_elements else \"\"\n",
    "        duration_element = box.select_one(\"div.year span:last-of-type\")\n",
    "        duration = duration_element.text.strip() if duration_element else \"\" \n",
    "        director_element = box.select_one(\"div.director a\")\n",
    "        director = director_element.text.strip() if director_element else \"\"\n",
    "        yd = box.select(\"div.year span\")\n",
    "        year2 = yd[0].text.strip()\n",
    "        duration2 = yd[1].text.strip()\n",
    "\n",
    "\n",
    "\n",
    "    # Create a dictionary to store the extracted data from this detail box\n",
    "        data_dict = {\n",
    "            'eng_title': eng_title,\n",
    "            'genre': genre,\n",
    "            'year': year,\n",
    "            'duration': duration,\n",
    "        }\n",
    "        \n",
    "        # Append the data_dict to the extracted_data list\n",
    "        extracted_data.append(data_dict)\n",
    "    \n",
    "    # Return the list containing all the extracted data from the HTML\n",
    "    return extracted_data\n",
    "\n",
    "\n",
    "# Folder path containing the HTML files\n",
    "folder_path = os.path.abspath('kmdb')\n",
    "\n",
    "# Get a list of file names in the 'kmdb' folder\n",
    "html_files = [f for f in os.listdir(folder_path) if f.endswith('.html')]\n",
    "\n",
    "# Loop through each HTML file in the 'kmdb' folder and process its content\n",
    "for file_name in html_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        html_content = file.read()\n",
    "        data = extract_data_from_html(html_content)\n",
    "        # Do something with the extracted data, e.g., store it in a list or database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd941fd",
   "metadata": {},
   "source": [
    "1. The `select` and `select_one` methods in BeautifulSoup are used to find elements that match a specified CSS selector. `select` returns a list of all elements that match the selector, while `select_one` returns only the first element that matches the selector. These methods are helpful when dealing with complex HTML structures where you need to find specific elements based on their class names, tags, or other attributes.\n",
    "\n",
    "2. In the provided HTML structure, there are multiple `<span>` elements with class \"national\" and \"country\". For example, `<span>다큐멘터리</span>` and `<span>대한민국</span>` are two separate `<span>` elements with class \"national\". Similarly, there are other elements with the same class name. When using `box.select(\"div.national span\")`, it returns a list of all the `<span>` elements that match the selector. To extract the text content of these elements, we use list indexing like `[0]` to access the first element, which corresponds to the genre, and `[1]` to access the second element, which corresponds to the country. The list indexing allows us to target specific elements from the list of matched elements.\n",
    "\n",
    "3. The `if genre_elements else \"\"` is a ternary operator in Python. It is used to provide a default value when `genre_elements` is an empty list. If `genre_elements` is not empty, it will take the value of `genre_elements[0].text.strip()`, which represents the text content of the first element in the list. If `genre_elements` is an empty list, it will take the value of an empty string `\"\"`. This is useful to avoid errors when trying to access elements from an empty list.\n",
    "\n",
    "4. The selector `\"div.year span:nth-of-type(2)\"` is used to target the second `<span>` element within the `<div>` element with class \"year\". The `nth-of-type(2)` is a CSS pseudo-class selector that matches the second occurrence of the specified element type within its parent. In this case, it is used to target the second `<span>` element inside the `<div>` with class \"year\". This second `<span>` element contains the duration information, which is extracted using `box.select_one(\"div.year span:nth-of-type(2)\")`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458fd3b6",
   "metadata": {},
   "source": [
    "### Apologies for the confusion in my previous response. I understand now why you encountered an `IndexError`. It seems that the HTML structure you provided is different from what I assumed. You can use list indexing to access the elements, but the issue is that there are only two `<span>` elements inside the `<div class=\"year\">`, not four as I initially mentioned.\n",
    "\n",
    "To extract the year and duration correctly using list indexing, you should use index `0` for the year and index `1` for the duration, like this:\n",
    "\n",
    "```python\n",
    "yd = box.select(\"div.year span\")\n",
    "year = yd[0].text.strip()\n",
    "duration = yd[1].text.strip()\n",
    "```\n",
    "\n",
    "The reason you encountered an `IndexError` is that you attempted to access the third and fourth elements of the `yd` list using index `3`, which don't exist in this case.\n",
    "\n",
    "Regarding your question about `nth-of-type(2)`, it is a CSS pseudo-class that can be used to select the second element of a particular type within its parent container. For example, if you had multiple `<div class=\"year\">` elements on the page, and you wanted to select the second one and then access its child `<span>` elements, you could use `nth-of-type(2)` as follows:\n",
    "\n",
    "```python\n",
    "second_year_div = b.select(\"div.year:nth-of-type(2)\")\n",
    "yd = second_year_div[0].select(\"span\")\n",
    "year = yd[0].text.strip()\n",
    "duration = yd[1].text.strip()\n",
    "```\n",
    "\n",
    "However, based on the HTML structure you provided, it seems that using `div.year span` with list indexing should work fine to access the year and duration elements.\n",
    "\n",
    "사실 이거 안 되고... 안 되는 이유는 `span`택이 얼핏 4개로 보여도 사실 첫번째, 세번째 태그는 `class=\"comma'`라서 쉼표로 생각됨. List indexing으로 접근되긴 되는데 사실 `span`이 2개 뿐인거라 [0], [1] 이렇게로만 접근할 수 있고 그러면 결과는 year:',', duration:2017년 이렇게밖에 안 나옴.. 그래서 무조건 `nth-of-type(2)` 이거 써야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "56f93845",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'eng_title': 'Pollock (Myeong-tae)',\n",
       "  'genre': '극영화',\n",
       "  'year': '2017년',\n",
       "  'duration': '23분'},\n",
       " {'eng_title': 'Sea Fog (Hae-mu)',\n",
       "  'genre': '극영화',\n",
       "  'year': '2014년',\n",
       "  'duration': '110분'},\n",
       " {'eng_title': 'Night Journey ( Yahaeng )',\n",
       "  'genre': '극영화',\n",
       "  'year': '2014년',\n",
       "  'duration': '14분'},\n",
       " {'eng_title': 'Sleeping Beauty',\n",
       "  'genre': '극영화',\n",
       "  'year': '2007년',\n",
       "  'duration': '109분'},\n",
       " {'eng_title': 'The Abortion',\n",
       "  'genre': '극영화',\n",
       "  'year': '2011년',\n",
       "  'duration': '32분'},\n",
       " {'eng_title': 'FAREWELL (Jak-byul-deul)',\n",
       "  'genre': '극영화',\n",
       "  'year': '2011년',\n",
       "  'duration': '93분'},\n",
       " {'eng_title': 'Tuning Fork',\n",
       "  'genre': '극영화',\n",
       "  'year': '2014년',\n",
       "  'duration': '105분'},\n",
       " {'eng_title': 'PHISHING (Pisingjakjeon)',\n",
       "  'genre': '극영화',\n",
       "  'year': '2021년',\n",
       "  'duration': '20분'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2442a97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
